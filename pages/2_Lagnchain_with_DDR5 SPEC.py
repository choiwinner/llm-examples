import streamlit as st
#PDF Loader
# PDF Loaderì‹œ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ 1Pageì”© ë¶ˆëŸ¬ì˜¤ê³  ì´ë¥¼ list.extendë¥¼ ì´ìš©í•´ì„œ ë¦¬ìŠ¤íŠ¸ í•˜ë‚˜ì”©(1Pageì”© ì¶”ê°€í•¨)
from langchain.document_loaders import PyPDFLoader #PyPDFLoader: ì£¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
from langchain.document_loaders import PyMuPDFLoader #PyMuPDFLoader:í…ìŠ¤íŠ¸ ì¶”ì¶œë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ì£¼ì„ ë“±ì˜ ì •ë³´ë„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤

from langchain.schema.runnable import RunnableMap
from langchain.retrievers.multi_query import MultiQueryRetriever

import google.generativeai as genai
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
import os
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI


#PyPDFLoader: ì£¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
def process_pdfs2():
    with st.spinner("PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
        #ì„ë² ë”© ëª¨ë¸ ë¶ˆë¡œì˜¤ê¸°
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ(allow_dangerous_deserialization=True í•„ìš”)
        vectorstore = FAISS.load_local("Data/DDR5_index2", embeddings, allow_dangerous_deserialization=True)
        #st.success(f"ì´ {len(pdf_docs)}íŒŒì¼, {len(data)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vectorstore

def get_conversation_chain(vectorstore,query):
   
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    template = """ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ChatBOTìœ¼ë¡œ Question ë‚´ìš©ì— ëŒ€í•´ì„œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
    ëŒ€ë‹µì€ Contextì— ìˆëŠ” ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
    ë˜ë„ë¡ì´ë©´ ìì„¸í•œ ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•˜ê³  contextì˜ ìˆëŠ” original sourceë„ ê°™ì´ ë³´ì—¬ì£¼ì„¸ìš”.
    #Context: 
    {context}
    #Question: 
    {question}

    #Answer:
    """

    prompt = ChatPromptTemplate.from_template(template)

    faiss_retriever=vectorstore.as_retriever(search_type="mmr",search_kwargs={'k':3, 'fetch_k': 10})

    retriever_from_llM = MultiQueryRetriever.from_llm(faiss_retriever, llm=llm)

    chain = RunnableMap({
        "context": lambda x: retriever_from_llM.get_relevant_documents(x['question']),
        "question": lambda x: x['question']
    }) | prompt | llm |StrOutputParser()

    response = chain.invoke({'question': query})


    return response

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(f"Human: {message.content}")
        else:
            st.write(f"AI: {message.content}")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_chat_history_str(chat_history):
    return "\n".join([f"{entry['role'].capitalize()}: {entry['content']}" for entry in chat_history])

def main():

    st.set_page_config(page_title="Lagnchain_with_pdf", page_icon=":books:")
    st.title("ğŸ¦œğŸ”— Lagnchain_with_pdf")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "pdf_input" not in st.session_state:
        st.session_state.pdf_input = None     

    with st.sidebar:
        gemini_api_key = st.text_input('Gemini_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.', key="langchain_search_api_gemini", type="password")
        "[Get an Gemini API key](https://ai.google.dev/)"
        "[How to get Gemini API Key](https://luvris2.tistory.com/880)"

        if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
            st.warning('ì˜ëª»ëœ key ì…ë ¥', icon='âš ï¸')
        else:
            st.success('ì •ìƒ key ì…ë ¥', icon='ğŸ‘‰')

        if process :=st.button("Process"):
            if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
                st.error("ì˜ëª»ëœ key ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                st.stop()

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state.conversation = None
            st.session_state.chat_history = []
            st.session_state.vectorstore = None
            st.session_state.pdf_input == None
            
    #0. gemini api key Setting
    if not gemini_api_key:
        st.warning("Gemini API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    genai.configure(api_key=gemini_api_key)

    #0. gemini api key Setting
    os.environ["GOOGLE_API_KEY"] = gemini_api_key

    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
    if st.session_state.vectorstore == None:
        # PDF íŒŒì¼ ì—…ë¡œë“œ(#accept_multiple_files=True optionì´ ìˆì–´ì•¼ ì—¬ëŸ¬ê°œì˜ pdfíŒŒì¼ ë™ì‹œ upload ê°€ëŠ¥)
        #pdf_docs = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

        #st.session_state.vectorstore = process_pdfs2(pdf_docs)
        st.session_state.vectorstore = process_pdfs2()

    # create conversation chain
        #st.session_state.conversation = get_conversation_chain(st.session_state.vectorstore)

    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['chat_history']ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
    if ("chat_history" in st.session_state) and (len(st.session_state['chat_history'])>0):
        #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
        for role, message in st.session_state['chat_history']: 
            st.chat_message(role).write(message)

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)
        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        st.session_state['chat_history'].append(('user',query))

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                # chain í˜¸ì¶œ
                response = get_conversation_chain(st.session_state.vectorstore,query)
                st.write(response)
                #6. response session_state 'assistant'ì— append í•œë‹¤.
                st.session_state['chat_history'].append(('assistant',response))

if __name__ == '__main__':
    main()
    

    



