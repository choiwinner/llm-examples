import streamlit as st
import pandas as pd

#PDF Loader
# PDF Loaderì‹œ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ 1Pageì”© ë¶ˆëŸ¬ì˜¤ê³  ì´ë¥¼ list.extendë¥¼ ì´ìš©í•´ì„œ ë¦¬ìŠ¤íŠ¸ í•˜ë‚˜ì”©(1Pageì”© ì¶”ê°€í•¨)
from langchain.document_loaders import PyPDFLoader #PyPDFLoader: ì£¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
from langchain.document_loaders import PyMuPDFLoader #PyMuPDFLoader:í…ìŠ¤íŠ¸ ì¶”ì¶œë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ì£¼ì„ ë“±ì˜ ì •ë³´ë„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤

from langchain.schema.runnable import RunnableMap
from langchain.retrievers.multi_query import MultiQueryRetriever

import google.generativeai as genai
from langchain.prompts import ChatPromptTemplate

from langchain.vectorstores import FAISS
from langchain_community.vectorstores.faiss import DistanceStrategy #vectorstoresì˜ ê±°ë¦¬ ê³„ì‚° ë°©ì‹ ê²°ì •
from langchain.retrievers import EnsembleRetriever # ì—¬ëŸ¬ retrieverë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì²˜ë¦¬
from langchain_community.retrievers import BM25Retriever  #TF-IDF ê³„ì—´ì˜ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜

from langchain.chains.question_answering import load_qa_chain
import os

from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

#import rank_bm25

import time

#PyPDFLoader: ì£¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
def process_pdfs(vector_distance_cal):
    with st.spinner("PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
        #ì„ë² ë”© ëª¨ë¸ ë¶ˆë¡œì˜¤ê¸°
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ(allow_dangerous_deserialization=True í•„ìš”)
        vectorstore = FAISS.load_local("Data/DRAM_index", 
        embeddings,
        distance_strategy=vector_distance_cal, 
        allow_dangerous_deserialization=True)
        #st.success(f"ì´ {len(pdf_docs)}íŒŒì¼, {len(data)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        df = pd.read_pickle('Data/DRAM_SPEC_data.pkl')
        data_list = df.to_list()

    return vectorstore,data_list

def get_conversation_chain(vectorstore,data_list,f_ratio,query):
   
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    template = """ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ChatBOTìœ¼ë¡œ Question ë‚´ìš©ì— ëŒ€í•´ì„œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
    ëŒ€ë‹µì€ Contextì— ìˆëŠ” ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
    ë˜ë„ë¡ì´ë©´ ìì„¸í•œ ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•˜ê³  contextì˜ ìˆëŠ” original sourceë„ ê°™ì´ ë³´ì—¬ì£¼ì„¸ìš”.
    #Context: 
    {context}
    #Question: 
    {question}

    #Answer:
    """

    prompt = ChatPromptTemplate.from_template(template)
    
    # initialize the # initialize the bm25 retriever(mmr ë°©ì‹:30ê°œ->10ê°œ)


    faiss_retriever=vectorstore.as_retriever(search_type="mmr",
    search_kwargs={'k':10, 'fetch_k': 30})

    # initialize the bm25 retriever(10ê°œ)
    bm25_retriever = BM25Retriever.from_documents(data_list)
    bm25_retriever.k = 10

    # initialize the ensemble retriever
    #retriever ê°€ì¤‘ì¹˜ ì„¤ì •(bm25:30% + faiss:70%)
    # ë¬¸ì„œ ê²°í•© ë°©ì‹ ì„¤ì •(default setting:combine_documents-ê²°í•©ëœ ë¬¸ì„œë“¤ì„ í•©ì¹˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘)
    ensemble_retriever_combine = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weights=[1-f_ratio, f_ratio] 
        ,retriever_type="combine_documents")
    
    multiqueryretriever = MultiQueryRetriever.from_llm(ensemble_retriever_combine, llm=llm)

    context_output_init = multiqueryretriever.get_relevant_documents(query)

    chain = RunnableMap({
        "context": lambda x: multiqueryretriever.get_relevant_documents(x['question']),
        "question": lambda x: x['question']
    }) | prompt | llm |StrOutputParser()

    #chain = RunnableMap({
    #    "context": lambda x: multiqueryretriever.get_relevant_documents(x['question']),
    #    "question": lambda x: x['question']
    #}) | prompt | llm |StrOutputParser()

    response = chain.invoke({'question': query})

    return response

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(f"Human: {message.content}")
        else:
            st.write(f"AI: {message.content}")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_chat_history_str(chat_history):
    return "\n".join([f"{entry['role'].capitalize()}: {entry['content']}" for entry in chat_history])

def main():

    #langsmith Setting
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com" 
    os.environ["LANGCHAIN_PROJECT"] = "pr-oily-offense-18"               #í‘œì‹œë  PJTëª… Setting
    os.environ["LANGCHAIN_API_KEY"] = "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"
    from langsmith import Client
    client = Client()

    st.set_page_config(page_title="Lagnchain_with_pdf", page_icon=":books:")
    st.title("ğŸ¦œğŸ”— Lagnchain_with_pdf")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "pdf_input" not in st.session_state:
        st.session_state.pdf_input = None 
    if "data_list" not in st.session_state:
        st.session_state.data_list = []    

    with st.sidebar:
        gemini_api_key = st.text_input('Gemini_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.', key="langchain_search_api_gemini", type="password")
        "[Get an Gemini API key](https://ai.google.dev/)"
        "[How to get Gemini API Key](https://luvris2.tistory.com/880)"

        if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
            st.warning('ì˜ëª»ëœ key ì…ë ¥', icon='âš ï¸')
        else:
            st.success('ì •ìƒ key ì…ë ¥', icon='ğŸ‘‰')

        if process :=st.button("Process"):
            if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
                st.error("ì˜ëª»ëœ key ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                st.stop()

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state.conversation = None
            st.session_state.chat_history = []
            st.session_state.vectorstore = None
            st.session_state.pdf_input == None
            st.session_state.data_list = []

        if Spec_name :=st.button("Spec_name ë³´ê¸°"):
            df = pd.read_csv("Data/DRAM_Spec_Names.csv",encoding='utf-8')
            st.dataframe(df)

        if faiss_ratio := st.slider("ensemble ratio?(faiss ë¹„ìœ¨)", 0, 100, 75):
            st.info(f"faiss ratio is {faiss_ratio}%")

        if vector_option_1 := st.selectbox("Select the vector distance cal method?",("MAX_INNER_PRODUCT", "DOT_PRODUCT", "EUCLIDEAN_DISTANCE"),):

            st.info(f"You selected: {vector_option_1}")
            
            if vector_option_1 == "MAX_INNER_PRODUCT":
                vector_distance_cal = DistanceStrategy.MAX_INNER_PRODUCT #ë‚´ì (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ìœ ì‚¬)
            if vector_option_1 == "DOT_PRODUCT":
                vector_distance_cal = DistanceStrategy.DOT_PRODUCT #ì ê³±(ë‚´ì ê³¼ ë™ì¼) 
            if vector_option_1 == "EUCLIDEAN_DISTANCE":
                vector_distance_cal = DistanceStrategy.EUCLIDEAN_DISTANCE #ìœ í´ë¦¬ë“œ ê±°ë¦¬(L2)
           
    #0. gemini api key Setting
    if not gemini_api_key:
        st.warning("Gemini API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    genai.configure(api_key=gemini_api_key)

    #0. gemini api key Setting
    os.environ["GOOGLE_API_KEY"] = gemini_api_key

    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
    if st.session_state.vectorstore == None:
        st.session_state.vectorstore,st.session_state.data_list = process_pdfs(vector_distance_cal)

    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['chat_history']ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
    if ("chat_history" in st.session_state) and (len(st.session_state['chat_history'])>0):
        #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
        for role, message in st.session_state['chat_history']: 
            st.chat_message(role).write(message)

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):

        start_time = time.time()

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)
        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        st.session_state['chat_history'].append(('user',query))

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                # chain í˜¸ì¶œ
                response = get_conversation_chain(st.session_state.vectorstore,
                                                  st.session_state.data_list,
                                                  float(faiss_ratio/100),
                                                  query)
                st.write(response)
                end_time = time.time()
                total_time = (end_time - start_time)
                st.info(f"ê²€ìƒ‰ ì†Œìš” ì‹œê°„: {total_time}ì´ˆ")
                #6. response session_state 'assistant'ì— append í•œë‹¤.
                st.session_state['chat_history'].append(('assistant',response))

if __name__ == '__main__':
    main()
    

    



