import streamlit as st
import google.generativeai as genai
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
import os
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
# PyPDFLoaderì‹œ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ 1Pageì”© ë¶ˆëŸ¬ì˜¤ê³  ì´ë¥¼ list.extendë¥¼ ì´ìš©í•´ì„œ ë¦¬ìŠ¤íŠ¸ í•˜ë‚˜ì”©(1Pageì”© ì¶”ê°€í•¨)
from langchain.document_loaders import PyPDFLoader
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain.chains import LLMChain
from langchain.schema.runnable import RunnableMap

def process_pdfs2(pdf_docs):
    with st.spinner("PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
        documents = []
        for uploaded_file in pdf_docs:
            # ì›ë˜ íŒŒì¼ ì´ë¦„ ìœ ì§€
            original_filename = uploaded_file.name

            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix=original_filename) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name

            # PyPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œ
            loader = PyPDFLoader(tmp_file_path)
            documents.extend(loader.load())

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)

            # 1000ìë¡œ ì²­í‚¹í•˜ê¸°
            text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 50)

            #splitterë¥¼ ì´ìš©í•œ ë¬¸ì„œ ì²­í‚¹
            data = text_splitter.split_documents(documents)

            #ì„ë² ë”© í•˜ê¸°
            #ì„ë² ë”© ëª¨ë¸
            embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
            vectorstore = FAISS.from_documents(data, embeddings)

            st.success(f"ì´ {len(pdf_docs)}íŒŒì¼, {len(data)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vectorstore

def get_conversation_chain(vectorstore):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    
    template = """You are an AI assistant specialized in answering questions based on the provided PDF document. 
    Use the following pieces of context to answer the human's question. 
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    ê·¸ë¦¬ê³  matadataë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€ì˜ sourceë„ ì•Œë ¤ì£¼ì„¸ìš”.

    {context}

    Human: {question}
    AI Assistant: """

    PROMPT = PromptTemplate(
        template=template, input_variables=["context", "question"]
    )
    
    memory = ConversationBufferWindowMemory(memory_key="chat_history", k=4) 
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": PROMPT}
    )
    return conversation_chain

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(f"Human: {message.content}")
        else:
            st.write(f"AI: {message.content}")


def main():

    st.set_page_config(page_title="Lagnchain_with_pdf", page_icon=":books:")
    st.title("ğŸ¦œğŸ”— Lagnchain_with_pdf")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state['chat_history'] = []
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "pdf_input" not in st.session_state:
        st.session_state.pdf_input = None 
    if "retriever_from_llM" not in st.session_state:
        st.session_state.retriever_from_llM = {}

    with st.sidebar:
        gemini_api_key = st.text_input('Gemini_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.', key="langchain_search_api_gemini", type="password")
        "[Get an Gemini API key](https://ai.google.dev/)"
        "[How to get Gemini API Key](https://luvris2.tistory.com/880)"

        if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
            st.warning('ì˜ëª»ëœ key ì…ë ¥', icon='âš ï¸')
        else:
            st.success('ì •ìƒ key ì…ë ¥', icon='ğŸ‘‰')

        if process :=st.button("Process"):
            if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
                st.error("ì˜ëª»ëœ key ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                st.stop()

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state.conversation = None
            st.session_state['chat_history'] = []
            st.session_state.vectorstore = None
            st.session_state.pdf_input = None
            st.session_state.retriever_from_llM = {}
            
    #0. gemini api key Setting
    if not gemini_api_key:
        st.warning("Gemini API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    genai.configure(api_key=gemini_api_key)

    #0. gemini api key Setting
    os.environ["GOOGLE_API_KEY"] = gemini_api_key

    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
    if st.session_state.vectorstore == None:
        # PDF íŒŒì¼ ì—…ë¡œë“œ(#accept_multiple_files=True optionì´ ìˆì–´ì•¼ ì—¬ëŸ¬ê°œì˜ pdfíŒŒì¼ ë™ì‹œ upload ê°€ëŠ¥)
        pdf_docs = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

        st.session_state.vectorstore = process_pdfs2(pdf_docs)
        
    
    retriever_from_llM = st.session_state.vectorstore.as_retriever(search_type="mmr",search_kwargs={'k':5, 'fetch_k': 10})

    template = """ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ChatBOTìœ¼ë¡œ Question ë‚´ìš©ì— ëŒ€í•´ì„œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
    ëŒ€ë‹µì€ Contextì— ìˆëŠ” ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
    ë˜ë„ë¡ì´ë©´ ìì„¸í•œ ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•˜ê³  contextì˜ ìˆëŠ” original sourceë„ ê°™ì´ ë³´ì—¬ì£¼ì„¸ìš”.
    Context: {context}
    Question: {question}
    """

    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query})
        st.chat_message("user").write(query)

    prompt = ChatPromptTemplate.from_template(template)

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    chain = RunnableMap({
        "context": lambda x: retriever_from_llM.get_relevant_documents(x['question']),
        "question": lambda x: x['question']
    }) | prompt | llm

    #1. st.session_state ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state['messages'] = [] #st.session_stateì— messagesê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
        #2.'assistant' iconìœ¼ë¡œ writeë¥¼ ì¶œë ¥í•œë‹¤.

    #ìœˆë„ìš° í¬ê¸° kë¥¼ ì§€ì •í•˜ë©´ ìµœê·¼ kê°œì˜ ëŒ€í™”ë§Œ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™”ëŠ” ì‚­ì œ
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferWindowMemory(memory_key="chat_history", k=4) 

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['messages']ê°€ ìˆê³  ê¸¸ì´ê°€ 0 ì´ìƒì´ë©´ ì‹¤í–‰ 
    if ("messages" in st.session_state) and (len(st.session_state['messages'])>0):
        for role, message in st.session_state['messages']:  #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
            st.chat_message(role).write(message)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # chain í˜¸ì¶œ
            response = chain.invoke({'question': query}).content
            st.write(response)
            st.session_state['messages'].append(('assistant',response))

























    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['chat_history']ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
    if ("chat_history" in st.session_state) and (len(st.session_state['chat_history'])>0):
        #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
        for role, message in st.session_state['chat_history']: 
            st.chat_message(role).write(message)

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)
        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        st.session_state['chat_history'].append(('user',query))

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                # chain í˜¸ì¶œ
                response = st.session_state.conversation({'question': query})
                st.write(response['answer'])
                #st.session_state['chat_history'].append(('assistant',response['answer']))

        

##2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
## st.session_state['messages']ê°€ ìˆê³  ê¸¸ì´ê°€ 0 ì´ìƒì´ë©´ ì‹¤í–‰ 
#if ("messages_pdf" in st.session_state) and (len(st.session_state['messages_pdf'])>0):
#    for role, message in st.session_state['messages_pdf']:  #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ #ìˆìŒ.
#        st.chat_message(role).write(message)
#
#
#from langchain_google_genai import ChatGoogleGenerativeAI
#
#llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
#
#from langchain.chains import ConversationalRetrievalChain
#conversation_chain = ConversationalRetrievalChain.from_llm(
#    llm=llm,
#    chain_type="stuff",
#    retriever=vectorstore.as_retriever(),
#)
#
##3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
#if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."): 
#
#    #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
#    st.chat_message("user").write(query)
#    #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
#    st.session_state['messages'].append(('user',query))
#
#    with st.chat_message("assistant"):
#        with st.spinner("Thinking..."):
#            # chain í˜¸ì¶œ
#            response = conversation_chain.run(question=query)
#            st.write(response)
#            st.session_state['messages'].append(('assistant',response))

if __name__ == '__main__':
    main()
    

    



