import streamlit as st

#PDF Loader
# PDF Loaderì‹œ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ 1Pageì”© ë¶ˆëŸ¬ì˜¤ê³  ì´ë¥¼ list.extendë¥¼ ì´ìš©í•´ì„œ ë¦¬ìŠ¤íŠ¸ í•˜ë‚˜ì”©(1Pageì”© ì¶”ê°€í•¨)
from langchain.document_loaders import PyPDFLoader #PyPDFLoader: ì£¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
from langchain.document_loaders import PyMuPDFLoader #PyMuPDFLoader:í…ìŠ¤íŠ¸ ì¶”ì¶œë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ì£¼ì„ ë“±ì˜ ì •ë³´ë„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤



import google.generativeai as genai
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
import os
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain

#PyMuPDFLoader:í…ìŠ¤íŠ¸ ì¶”ì¶œë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ì£¼ì„ ë“±ì˜ ì •ë³´ë„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤
def process_pdfs(pdf_docs,pdf_model):
    with st.spinner("PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
        documents = []
        for uploaded_file in pdf_docs:
            # ì›ë˜ íŒŒì¼ ì´ë¦„ ìœ ì§€
            original_filename = uploaded_file.name

            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix=original_filename) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name

            if pdf_model == 'PyMuPDFLoader':
                # PyMuPDFLoader ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œ
                loader = PyMuPDFLoader(tmp_file_path)
            if pdf_model == 'PyPDFLoader':
                #PyPDFLoader ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œ
                loader = PyPDFLoader(tmp_file_path)

            documents.extend(loader.load())

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)

            # 1000ìë¡œ ì²­í‚¹í•˜ê¸°
            text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 50)

            #splitterë¥¼ ì´ìš©í•œ ë¬¸ì„œ ì²­í‚¹
            data = text_splitter.split_documents(documents)

            #ì„ë² ë”© í•˜ê¸°
            #ì„ë² ë”© ëª¨ë¸
            embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
            vectorstore = FAISS.from_documents(documents=data, embedding=embeddings)

            st.success(f"ì´ {len(pdf_docs)}íŒŒì¼, {len(data)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vectorstore

#PyPDFLoader: ì£¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
def process_pdfs2(pdf_docs):
    with st.spinner("PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
        documents = []
        for uploaded_file in pdf_docs:
            # ì›ë˜ íŒŒì¼ ì´ë¦„ ìœ ì§€
            original_filename = uploaded_file.name

            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix=original_filename) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name

            # PyPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œ
            loader = PyPDFLoader(tmp_file_path)
            documents.extend(loader.load())

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)

            # 1000ìë¡œ ì²­í‚¹í•˜ê¸°
            text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 50)

            #splitterë¥¼ ì´ìš©í•œ ë¬¸ì„œ ì²­í‚¹
            data = text_splitter.split_documents(documents)

            #ì„ë² ë”© í•˜ê¸°
            #ì„ë² ë”© ëª¨ë¸
            embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
            vectorstore = FAISS.from_documents(documents=data, embedding=embeddings)

            st.success(f"ì´ {len(pdf_docs)}íŒŒì¼, {len(data)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vectorstore

def get_conversation_chain(vectorstore):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    template = """ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ChatBOTìœ¼ë¡œ Question ë‚´ìš©ì— ëŒ€í•´ì„œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
    ëŒ€ë‹µì€ Contextì— ìˆëŠ” ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
    ë˜ë„ë¡ì´ë©´ ìì„¸í•œ ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•˜ê³  contextì˜ ìˆëŠ” original sourceë„ ê°™ì´ ë³´ì—¬ì£¼ì„¸ìš”.
    Context: {context}
    Question: {question}
    """
    PROMPT = PromptTemplate(
        template=template, input_variables=["context", "question"]
    )
    
    memory = ConversationBufferWindowMemory(memory_key="chat_history", k=3) 
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_type="mmr",search_kwargs={'k':3, 'fetch_k': 10}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": PROMPT}
    )
    return conversation_chain

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(f"Human: {message.content}")
        else:
            st.write(f"AI: {message.content}")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_chat_history_str(chat_history):
    return "\n".join([f"{entry['role'].capitalize()}: {entry['content']}" for entry in chat_history])

def main():

    st.set_page_config(page_title="Lagnchain_with_pdf", page_icon=":books:")
    st.title("ğŸ¦œğŸ”— Lagnchain_with_pdf")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "pdf_input" not in st.session_state:
        st.session_state.pdf_input = None     

    with st.sidebar:
        gemini_api_key = st.text_input('Gemini_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.', key="langchain_search_api_gemini", type="password")
        "[Get an Gemini API key](https://ai.google.dev/)"
        "[How to get Gemini API Key](https://luvris2.tistory.com/880)"

        if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
            st.warning('ì˜ëª»ëœ key ì…ë ¥', icon='âš ï¸')
        else:
            st.success('ì •ìƒ key ì…ë ¥', icon='ğŸ‘‰')

        if process :=st.button("Process"):
            if (gemini_api_key[0:2] != 'AI') or (len(gemini_api_key) != 39):
                st.error("ì˜ëª»ëœ key ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                st.stop()

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state.conversation = None
            st.session_state.chat_history = []
            st.session_state.vectorstore = None
            st.session_state.pdf_input == None

        #PDF Loader Model ì„ íƒ
        st.subheader('Gemini Modelì„ ì„ íƒí•˜ì„¸ìš”.')
        selected_model = st.sidebar.selectbox('Choose PDF Model', ['PyPDFLoader', 'PyMuPDFLoader'], key='selected_model')

            
    #0. gemini api key Setting
    if not gemini_api_key:
        st.warning("Gemini API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    genai.configure(api_key=gemini_api_key)

    #0. gemini api key Setting
    os.environ["GOOGLE_API_KEY"] = gemini_api_key

    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
    if st.session_state.vectorstore == None:
        # PDF íŒŒì¼ ì—…ë¡œë“œ(#accept_multiple_files=True optionì´ ìˆì–´ì•¼ ì—¬ëŸ¬ê°œì˜ pdfíŒŒì¼ ë™ì‹œ upload ê°€ëŠ¥)
        pdf_docs = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)

        #st.session_state.vectorstore = process_pdfs2(pdf_docs)
        st.session_state.vectorstore = process_pdfs(pdf_docs,selected_model)

    # create conversation chain
        st.session_state.conversation = get_conversation_chain(st.session_state.vectorstore)

    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['chat_history']ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
    if ("chat_history" in st.session_state) and (len(st.session_state['chat_history'])>0):
        #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
        for role, message in st.session_state['chat_history']: 
            st.chat_message(role).write(message)

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)
        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        #st.session_state['chat_history'].append(('user',query))

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                chat_history_str = get_chat_history_str(st.session_state.chat_history)
                # chain í˜¸ì¶œ
                #inputs = {'question': query, 'chat_history': st.session_state.chat_history}
                inputs = {'question': query, 'chat_history': chat_history_str}
                response = st.session_state.conversation(inputs)
                st.session_state.chat_history.append({"role": "human", "content": query})
                st.session_state.chat_history.append({"role": "ai", "content": response['answer']})
                st.write(response['answer'])



        

##2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
## st.session_state['messages']ê°€ ìˆê³  ê¸¸ì´ê°€ 0 ì´ìƒì´ë©´ ì‹¤í–‰ 
#if ("messages_pdf" in st.session_state) and (len(st.session_state['messages_pdf'])>0):
#    for role, message in st.session_state['messages_pdf']:  #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ #ìˆìŒ.
#        st.chat_message(role).write(message)
#
#
#from langchain_google_genai import ChatGoogleGenerativeAI
#
#llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
#
#from langchain.chains import ConversationalRetrievalChain
#conversation_chain = ConversationalRetrievalChain.from_llm(
#    llm=llm,
#    chain_type="stuff",
#    retriever=vectorstore.as_retriever(),
#)
#
##3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
#if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."): 
#
#    #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
#    st.chat_message("user").write(query)
#    #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
#    st.session_state['messages'].append(('user',query))
#
#    with st.chat_message("assistant"):
#        with st.spinner("Thinking..."):
#            # chain í˜¸ì¶œ
#            response = conversation_chain.run(question=query)
#            st.write(response)
#            st.session_state['messages'].append(('assistant',response))

if __name__ == '__main__':
    main()
    

    



